{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfDoE/LFEGg4TozKw94xxy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishika24-tech/CAFE-MANAGEMENT/blob/main/Rishikasaxena_sentimentanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qseuHSqVyoZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Function to list all files under a directory\n",
        "def list_files(directory):\n",
        "    for dirname, _, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            print(os.path.join(dirname, filename))\n",
        "\n",
        "# Assuming the input directory is \"../input/\"\n",
        "input_directory = \"../input\"\n",
        "list_files(input_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "UsyE0Z8HWOIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a string of words\n",
        "sentence = \"The cat sat on the mat\"\n",
        "\n",
        "# Split the sentence into a list of words\n",
        "word_list = sentence.split()\n",
        "\n",
        "# Print the list of words\n",
        "print(word_list)\n"
      ],
      "metadata": {
        "id": "3nuF3bBGXRrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "train_df = pd.read_csv(\"/path/to/your/twitter_training.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "zOWjob1TXUxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique values in the '2401' column of the DataFrame\n",
        "unique_values_count = len(train_df['2401'].unique())\n",
        "\n",
        "# Print the count of unique values\n",
        "print(\"Number of unique values in column '2401':\", unique_values_count)\n"
      ],
      "metadata": {
        "id": "yJvPfsOuZml2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the column named '2401' from the DataFrame\n",
        "train_df = train_df.drop(\"2401\", axis=1)\n",
        "\n",
        "# Display the modified DataFrame\n",
        "train_df\n"
      ],
      "metadata": {
        "id": "i8dGxsDVabyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "id": "Vls0RruxahzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary to rename columns\n",
        "column_mapping = {'Borderlands': 'Entity',\n",
        "                  'Positive': 'Sentiment',\n",
        "                  'im getting on borderlands and i will murder you all ,': 'Content'}\n",
        "\n",
        "# Rename columns using the defined dictionary\n",
        "train_df = train_df.rename(columns=column_mapping)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "YVWaxb97ajNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from the PyTorch library\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "vekNDzpkbQ81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, text_data):\n",
        "        self.text_data = text_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.text_data[index]\n"
      ],
      "metadata": {
        "id": "c22gKMbGbekO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary module from torchtext library\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Create a tokenizer using basic English tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "# Tokenize the input sentence\n",
        "tokens = tokenizer(\"I am reading a book now. I love to read books!\")\n",
        "\n",
        "# Print the tokens\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "ixwVYzvycFDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    # Create a tokenizer using basic English tokenizer\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "    # Tokenize the input text\n",
        "    tokens = tokenizer(text)\n",
        "\n",
        "    # Return the tokens\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "MKRxE7fBcUKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary module from NLTK library\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords dataset\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "EzuxxXRJdJjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the set of stopwords to a list and print the first 5 elements\n",
        "stop_words_list = list(stop_words)[:5]\n",
        "print(stop_words_list)\n"
      ],
      "metadata": {
        "id": "vyzTjWyidX2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out tokens that are stopwords\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Print the filtered tokens\n",
        "filtered_tokens\n"
      ],
      "metadata": {
        "id": "UBlUZh3id5cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(tokens):\n",
        "    # Get the set of English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Filter out tokens that are stopwords\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Return the filtered tokens\n",
        "    return filtered_tokens\n"
      ],
      "metadata": {
        "id": "dx5UJPgneQvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary module from NLTK library\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Create a PorterStemmer object\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# List of input tokens\n",
        "inp = [\"Baari\", \"had\", \"little\", \"lamp\", \"exploded\", \"after\", \"power\", \"surge\"]\n",
        "\n",
        "# Stem each token in the input list\n",
        "stemmed_tokens = [stemmer.stem(token) for token in inp]\n",
        "\n",
        "# Print the stemmed tokens\n",
        "stemmed_tokens\n"
      ],
      "metadata": {
        "id": "Ncjty2OxelGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_tokens(tokens):\n",
        "    # Create a PorterStemmer object\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Stem each token in the input list\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Return the stemmed tokens\n",
        "    return stemmed_tokens\n"
      ],
      "metadata": {
        "id": "bV6j9Mw7fDIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_processing_pipeline(text):\n",
        "    # Tokenize the input text\n",
        "    tokens = tokenize_text(text)\n",
        "\n",
        "    # Remove stopwords from the tokens\n",
        "    tokens = remove_stop_words(tokens)\n",
        "\n",
        "    # Stem the tokens\n",
        "    tokens = stem_tokens(tokens)\n",
        "\n",
        "    # Implement rare word removal or other processing steps if needed\n",
        "\n",
        "    # Create a dataset from the tokens (not implemented in this function)\n",
        "    dataset = TwitterDataset(tokens)\n",
        "\n",
        "    # Create a dataloader from the dataset (not implemented in this function)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "xXW_49wzfWuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary module from the PyTorch library\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "NBE-bRgXgJwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentAnalysisCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # EMBEDDING LAYER\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # CONVOLUTION LAYER\n",
        "        # We use 1D convolution for text data\n",
        "        self.conv1d = nn.Conv1d(in_channels=embed_dim, out_channels=vocab_size, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # FULLY CONNECTED (LINEAR) LAYER\n",
        "        self.fc = nn.Linear(vocab_size, 2)  # Output size is 2 for binary sentiment analysis\n",
        "\n",
        "    def forward(self, text):\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding(text).permute(0, 2, 1)  # Permute dimensions for convolution\n",
        "\n",
        "        # Convolution layer\n",
        "        conved = self.conv1d(embedded)\n",
        "        conved = F.relu(conved)\n",
        "\n",
        "        # Global max pooling\n",
        "        pooled = F.max_pool1d(conved, kernel_size=conved.shape[2]).squeeze(2)\n",
        "\n",
        "        # Fully connected layer\n",
        "        output = self.fc(pooled)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "j7pUi4eSgzYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "Uara9e3khMKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the SentimentAnalysisCNN model\n",
        "model = SentimentAnalysisCNN(vocab_size=1000, embed_dim=10)\n",
        "\n",
        "# Define the loss function (cross entropy loss)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer (Stochastic Gradient Descent with learning rate 0.1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n"
      ],
      "metadata": {
        "id": "p3RRpvOzhmlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the content and sentiment of the first row in the DataFrame\n",
        "content = train_df['Content'][0]\n",
        "sentiment = train_df['Sentiment'][0].lower()\n",
        "\n",
        "# Print the content and sentiment\n",
        "print(content, sentiment)\n"
      ],
      "metadata": {
        "id": "Dbz2vW1fh-3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the 'Content' column of the DataFrame and display the first few rows\n",
        "content_column = train_df['Content']\n",
        "content_column.head()\n"
      ],
      "metadata": {
        "id": "VmKyVSxLigWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the 'Sentiment' column of the DataFrame and display the first few rows\n",
        "sentiment_column = train_df['Sentiment']\n",
        "sentiment_column.head()\n"
      ],
      "metadata": {
        "id": "QO_qsWsaiwcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, b = train_df['Content'], train_df['Sentiment']\n"
      ],
      "metadata": {
        "id": "KRxoyk5sjHoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length_of_a = len(train_df['Content'])\n"
      ],
      "metadata": {
        "id": "JkY-YzIsjI2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = set(word for tweet in train_df['Content'] for word in str(tweet).split())\n",
        "number_of_unique_words = len(unique_words)\n"
      ],
      "metadata": {
        "id": "6WTyuOZGjrvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_unique_words\n"
      ],
      "metadata": {
        "id": "LMaec9BzkL_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_list_to_text(list_data, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for item in list_data:\n",
        "            file.write(f\"{item}\\n\")  # Write each item with a newline character\n",
        "\n",
        "# Example usage\n",
        "input_list = unique_words\n",
        "folder_path = \"/kaggle/working/\"\n",
        "filename = \"vocab.txt\"\n",
        "\n",
        "save_list_to_text(input_list, folder_path + filename)\n"
      ],
      "metadata": {
        "id": "kS0YdU98kegv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming unique_words is already defined as the list of unique words\n",
        "word_to_idx = {w: idx for idx, w in enumerate(unique_words)}\n"
      ],
      "metadata": {
        "id": "wpEeFn0Vk7kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for word, index in word_to_idx.items():\n",
        "    print(word, index)\n",
        "    count += 1\n",
        "    if count > 5:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "VMkviThZoF8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "    for sentence, label in zip(a, b):\n",
        "        model.zero_grad()\n",
        "        sentence_indices = torch.LongTensor([word_to_idx.get(word, 0) for word in sentence]).unsqueeze(0)\n",
        "        outputs = model(sentence_indices)\n",
        "        label_tensor = torch.LongTensor([int(label)])\n",
        "        loss = criterion(outputs, label_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n"
      ],
      "metadata": {
        "id": "HaBiompRoMPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment classification pipeline\n",
        "sentiment_classifier = pipeline(\"text-classification\")\n"
      ],
      "metadata": {
        "id": "N7ZiLZm8oP-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_first_row = train_df.loc[0, \"Content\"]\n",
        "sentiment_first_row = train_df.loc[0, \"Sentiment\"].lower()\n"
      ],
      "metadata": {
        "id": "Iib3MY8RorCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique values in the 'Sentiment' column of the DataFrame\n",
        "unique_sentiments = train_df['Sentiment'].unique()\n"
      ],
      "metadata": {
        "id": "JJkAjrN7o4gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shape of the DataFrame\n",
        "df_shape = train_df.shape\n"
      ],
      "metadata": {
        "id": "86lBzsLppMRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame to include only rows with 'Positive' or 'Negative' sentiment\n",
        "PN_train = train_df.query(\"Sentiment == 'Positive' or Sentiment == 'Negative'\")\n",
        "\n",
        "# Get the unique values in the 'Sentiment' column of the filtered DataFrame\n",
        "unique_sentiments = PN_train['Sentiment'].unique()\n"
      ],
      "metadata": {
        "id": "btcPdNGspZjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the DataFrame to include only rows with 'Positive' or 'Negative' sentiment\n",
        "PN_train = train_df[train_df['Sentiment'].isin(['Positive', 'Negative'])]\n",
        "\n",
        "# Get the shape of the filtered DataFrame\n",
        "shape_PN_train = PN_train.shape\n"
      ],
      "metadata": {
        "id": "ykzMIs-QpsCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform sentiment classification on the input text\n",
        "outputs = sentiment_classifier(\"I am so happy right now\")\n",
        "\n",
        "# Extract the predicted sentiment label from the outputs\n",
        "predicted_sentiment = outputs[0][\"label\"].lower()\n",
        "\n",
        "# Print the predicted sentiment\n",
        "print(predicted_sentiment)\n"
      ],
      "metadata": {
        "id": "WovQlz-4p9xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of rows in the filtered DataFrame\n",
        "num_rows_PN_train = len(PN_train)\n"
      ],
      "metadata": {
        "id": "5nGCapebqQX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the first 5 rows of the filtered DataFrame\n",
        "first_5_rows_PN_train = PN_train.head(5)\n"
      ],
      "metadata": {
        "id": "hQ6OlwVJqjwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the content of rows 10 to 19 (inclusive) from the filtered DataFrame\n",
        "content_subset_PN_train = PN_train.loc[10:19, 'Content']\n"
      ],
      "metadata": {
        "id": "00WMhyEAqqbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index of the filtered DataFrame and drop the old index column\n",
        "PN_train = PN_train.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "QyIAj9DhrGyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "correct = 0\n",
        "\n",
        "# Loop through the first 2000 rows of PN_train\n",
        "for content, sentiment in zip(PN_train['Content'][:2000], PN_train['Sentiment'][:2000].str.lower()):\n",
        "    # Convert content to string to handle float values\n",
        "    content = str(content)\n",
        "\n",
        "    # Perform sentiment classification on the content\n",
        "    pred = sentiment_classifier(content)[0][\"label\"].lower()\n",
        "\n",
        "    # Check if the predicted sentiment matches the actual sentiment\n",
        "    if sentiment == pred:\n",
        "        correct += 1\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    # Print progress every 100 iterations\n",
        "    if count % 100 == 0 and count != 0:\n",
        "        accuracy = correct / count * 100\n",
        "        print(f\"{count} | Correct: {correct} | Percentage: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "9_EC7ANHreEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import datasets\n",
        "\n",
        "# Load the AutoModelForSequenceClassification and AutoTokenizer classes\n",
        "AutoModelForSequenceClassification = transformers.AutoModelForSequenceClassification\n",
        "AutoTokenizer = transformers.AutoTokenizer\n",
        "\n",
        "# Load the load_dataset function from the datasets module\n",
        "load_dataset = datasets.load_dataset\n"
      ],
      "metadata": {
        "id": "7YQ08bnJsQA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Yeukiur2sprO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the IMDb dataset\n",
        "data = load_dataset(\"imdb\")\n"
      ],
      "metadata": {
        "id": "ufaDJDSqtBSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the 'Entity' column from the DataFrame inplace\n",
        "train_df.drop(columns=['Entity'], inplace=True)\n"
      ],
      "metadata": {
        "id": "GuSr43rytNB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of missing values in each column of the DataFrame\n",
        "na_count = train_df.isna().sum()\n"
      ],
      "metadata": {
        "id": "EjBwWwmQtbYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values and reset index inplace\n",
        "train_df.dropna(inplace=True)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Drop the 'index' column inplace\n",
        "train_df.drop(columns=['index'], inplace=True)\n"
      ],
      "metadata": {
        "id": "E5nQ9ZBitrUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the inplace parameter of the rename method\n",
        "train_df = train_df.rename(columns={\"Sentiment\": \"label\", \"Content\": \"text\"})\n"
      ],
      "metadata": {
        "id": "bZq9-t_muAG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reindex columns inplace\n",
        "train_df = train_df[['text', 'label']]\n"
      ],
      "metadata": {
        "id": "xePrf1UyuLqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "print(train_df.head())\n"
      ],
      "metadata": {
        "id": "rtQXitr_uc_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Convert pandas DataFrame to Dataset\n",
        "data = Dataset.from_pandas(train_df)\n"
      ],
      "metadata": {
        "id": "Bvx-MVG4vMPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenize function to the dataset in batches\n",
        "data = data.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "zOG43xW9vXWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to store tokenized data\n",
        "tokenized_data = []\n",
        "\n",
        "# Loop through the 'Content' column of the dataset\n",
        "for idx, content in enumerate(data[\"Content\"]):\n",
        "    # Print progress every 5000 iterations\n",
        "    if idx % 5000 == 0:\n",
        "        print(\"Iteration\", idx)\n",
        "    # Tokenize the content\n",
        "    tokenized_content = tokenize(content)\n",
        "    # Append tokenized content to the list\n",
        "    tokenized_data.append(tokenized_content)\n"
      ],
      "metadata": {
        "id": "-ZqOQseMvm-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first two elements of the tokenized data list\n",
        "tokenized_data[:2]\n"
      ],
      "metadata": {
        "id": "3Jl56RDFvzMB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}